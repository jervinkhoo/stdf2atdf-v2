# src/converter.py
import logging
from typing import Optional, List, Dict, Any
from pydantic import BaseModel, ValidationError

# from .core.utils.files import managed_files # Old import
#from .core.stdf.preprocessing import determine_file_params, read_record_header
# Imports moved to stdf_parser module
from .core.stdf_parser.handler import initialize_record_entries, setup_record_flags, determine_file_params, \
    read_record_header, handle_stdf_entry # Changed from handle_stdf_entries
from .core.stdf_parser.templates import create_stdf_mapping, get_stdf_template # Moved STDF template functions
# Imports from new utils location
from .utils.files import validate_input_file, managed_files # Added managed_files here
from .utils.decorators import timing_decorator
# Database import removed/commented previously
# from .core.utils.database import create_database_from_atdf
# ATDF imports - REMOVE old handler, ADD new generator handler parts
# from .core.atdf.handler import handle_atdf_entries # REMOVE THIS
from .core.atdf_generator.handler import handle_atdf_entry, write_atdf_file # ADD THIS
from .core.atdf_generator.templates import get_atdf_template # Import from new location
from .core.data_transformers.record_modifiers.base import modify_record # Renamed import
from .core.data_transformers.id_enricher import add_hierarchical_ids

# try:
#     import django
#
#     django_available = True
# except ImportError:
#     django_available = False

logger = logging.getLogger(__name__)


class RecordProcessingContext(BaseModel):
    data: Optional[bytes] = None
    endianness: str
    stdf_template: Dict[str, Any] # Modified in place by handle_stdf_entry, used by handle_atdf_entry
    atdf_template: Dict[str, Any]
    stdf_file: Any  # File-like object
    atdf_file: Optional[Any] = None # File-like object, can be None
    modifier_type: Optional[str] = None  # Renamed from preprocessor_type
    # counters, stdf_processed_entries, and atdf_processed_entries removed
    # always_return_atdf_dict removed as it's implicit or handled by caller

    class Config:
        arbitrary_types_allowed = True


def process_record(
    context: RecordProcessingContext,
    stdf_processed_entries: Dict[str, List[Dict[str, Any]]],
    atdf_processed_entries: Dict[str, List[Dict[str, Any]]],
    counters: Dict[str, int]
) -> None:
    """
    Process a single STDF record.
    Phase 3 Change: This now only handles the STDF part and calls the new ATDF generator
                    for dictionary creation and file writing if needed.
                    Preprocessing and ID enrichment are temporarily bypassed here and
                    will be integrated in later phases via converter_service.py.
    """
    # 1. Process STDF data (get STDF dictionary representation)
    #    handle_stdf_entry parses the data and updates context.stdf_template['fields'] with values.
    #    The result is then appended to stdf_processed_entries[record_type].
    if context.data: # Check if data exists (e.g., not EPS)
        # Call handle_stdf_entry directly
        parsed_stdf_record = handle_stdf_entry(
            context.stdf_template, # This template is modified by handle_stdf_entry
            context.data,
            context.endianness
        )
        # Append the parsed STDF record to the stdf_processed_entries collection
        # This step was previously inside handle_stdf_entries
        stdf_record_type = context.stdf_template.get('record_type', 'Unknown') # Get record_type for stdf collection
        if stdf_record_type not in stdf_processed_entries:
            stdf_processed_entries[stdf_record_type] = []
        stdf_processed_entries[stdf_record_type].append(parsed_stdf_record)

    # 2. Generate base ATDF dictionary (using the new ATDF generator handler)
    #    This uses the values populated in context.stdf_template by the call to handle_stdf_entry above.
    base_atdf_entry = handle_atdf_entry(context.atdf_template, context.stdf_template)

    # 3. Apply Modifier
    # Get record_type for modifier, enricher, and atdf collection from atdf_template
    atdf_record_type = context.atdf_template['record_type']
    modified_atdf_entry = modify_record( # Renamed function call
        atdf_record_type,
        base_atdf_entry.copy(), # Pass a copy to avoid modifying base_atdf_entry if it's used elsewhere
        context.modifier_type # Access directly, Pydantic handles None if optional
    )

    # 4. Write to ATDF file if requested (using the MODIFIED entry)
    if context.atdf_file: # Access directly
        write_atdf_file(context.atdf_file, modified_atdf_entry, context.atdf_template)

    # 5. Apply ID Enrichment (modifies modified_atdf_entry in place and returns it)
    enriched_atdf_entry = add_hierarchical_ids(
        atdf_record_type,
        modified_atdf_entry, # Pass the (potentially) modified dictionary
        atdf_processed_entries, # The main collection for context
        counters # Pass counters directly
    )

    # 6. Append the enriched entry to the main ATDF collection
    if atdf_record_type not in atdf_processed_entries: # Ensure list exists
        atdf_processed_entries[atdf_record_type] = []
    atdf_processed_entries[atdf_record_type].append(enriched_atdf_entry)
    logger.debug(f"Appended to {atdf_record_type}: {enriched_atdf_entry}. "
                 f"Total {atdf_record_type} entries: {len(atdf_processed_entries[atdf_record_type])}")


@timing_decorator
def run_conversion(
        stdf_input_file: str, # Renamed for clarity from services.py call
        atdf_output_file: Optional[str] = None, # Renamed for clarity
        # output_atdf_database parameter removed
        records_to_process: Optional[List[str]] = None, # Type hint updated
        modifier_type: Optional[str] = None  # Renamed from preprocessor_type
        # counters parameter will be added when add_hierarchical_ids is implemented
) -> Dict[str, List[Dict]]:
    """
    Run STDF to ATDF conversion.
    Optionally writes to an ATDF file.
    Always returns a dictionary containing the processed ATDF entries, keyed by record type.
    """
    validate_input_file(stdf_input_file)

    stdf_mapping = create_stdf_mapping()
    stdf_processed_entries = initialize_record_entries()
    atdf_processed_entries = initialize_record_entries()
    record_flags = setup_record_flags(records_to_process)
    
    # Initialize counters for w_id and p_id generation for the current file
    counters: Dict[str, int] = {'w_counter': 0, 'p_counter': 0}

    try:
        with managed_files(stdf_input_file, atdf_output_file) as (stdf_file, atdf_file_handle):
            file_params = determine_file_params(stdf_file)

            while True:
                header_data = read_record_header(stdf_file, file_params['endianness'])
                if not header_data:
                    break

                rec_len, rec_typ, rec_sub = header_data
                data = stdf_file.read(rec_len)

                if len(data) < rec_len:
                    logger.error(f"Incomplete record data: expected {rec_len} bytes, got {len(data)}")
                    continue

                try:
                    stdf_template = get_stdf_template(stdf_mapping, rec_typ, rec_sub)
                    record_type = stdf_template['record_type']

                    if not record_flags.get(record_type, False):
                        continue

                    atdf_template = get_atdf_template(record_type)

                    context = RecordProcessingContext(
                        data=data,
                        endianness=file_params['endianness'],
                        stdf_template=stdf_template, # This will be modified by handle_stdf_entry
                        atdf_template=atdf_template,
                        stdf_file=stdf_file,
                        atdf_file=atdf_file_handle,
                        modifier_type=modifier_type  # Renamed from preprocessor_type
                    )
                    process_record(
                        context=context,
                        stdf_processed_entries=stdf_processed_entries,
                        atdf_processed_entries=atdf_processed_entries,
                        counters=counters
                    )

                except ValidationError as ve:
                    logger.error(f"Pydantic ValidationError creating context for record: {ve.errors()}")
                    # Optionally, log the problematic data: logger.error(f"Problematic context_data: {context_data}")
                    continue
                except Exception as e:
                    logger.error(f"Generic error processing record: {e}", exc_info=True) # Add exc_info for more details
                    continue
        # Database creation logic removed.

        logger.info(f"Successfully processed {stdf_input_file}")
        # Return the processed ATDF entries
        # WARNING: As noted in process_record, this dictionary will be incomplete
        #          during Phase 3 as enrichment is bypassed. It will be corrected in Phase 4/5.
        # logger.debug(f"Final atdf_processed_entries before return: {atdf_processed_entries}") # Removed for less verbose logging
        return atdf_processed_entries

    except Exception as e:
        logger.exception(f"Fatal error during conversion of {stdf_input_file}: {e}")
        # Re-raise the exception to signal failure clearly.
        raise
